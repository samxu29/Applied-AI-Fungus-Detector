{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samxu/documents/apply_ai/final_fungus-detector/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.1+cu118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/facebookresearch/deit/zipball/main\" to /home/samxu/.cache/torch/hub/main.zip\n",
      "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_base_patch16_224-b5f2ef4d.pth\" to /home/samxu/.cache/torch/hub/checkpoints/deit_base_patch16_224-b5f2ef4d.pth\n",
      "100%|██████████| 330M/330M [00:08<00:00, 43.0MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "import timm\n",
    "import requests\n",
    "import torchvision.transforms as transforms\n",
    "from timm.data.constants import IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD\n",
    "\n",
    "print(torch.__version__)\n",
    "# should be 1.8.0\n",
    "\n",
    "\n",
    "model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256, interpolation=3),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(IMAGENET_DEFAULT_MEAN, IMAGENET_DEFAULT_STD),\n",
    "])\n",
    "\n",
    "img = Image.open(requests.get(\"https://raw.githubusercontent.com/pytorch/ios-demo-app/master/HelloWorld/HelloWorld/HelloWorld/image.png\", stream=True).raw)\n",
    "img = transform(img)[None,]\n",
    "out = model(img)\n",
    "clsidx = torch.argmax(out)\n",
    "print(clsidx.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/samxu/.cache/torch/hub/facebookresearch_deit_main\n"
     ]
    }
   ],
   "source": [
    "model = torch.hub.load('facebookresearch/deit:main', 'deit_base_patch16_224', pretrained=True)\n",
    "model.eval()\n",
    "scripted_model = torch.jit.script(model)\n",
    "scripted_model.save(\"demo_weight/fbdeit_scripted.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/samxu/documents/apply_ai/final_fungus-detector/.venv/lib/python3.10/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Use 'x86' for server inference (the old 'fbgemm' is still available but 'x86' is the recommended default) and ``qnnpack`` for mobile inference.\n",
    "backend = \"x86\" # replaced with ``qnnpack`` causing much worse inference speed for quantized model on this notebook\n",
    "model.qconfig = torch.quantization.get_default_qconfig(backend)\n",
    "torch.backends.quantized.engine = backend\n",
    "\n",
    "quantized_model = torch.quantization.quantize_dynamic(model, qconfig_spec={torch.nn.Linear}, dtype=torch.qint8)\n",
    "scripted_quantized_model = torch.jit.script(quantized_model)\n",
    "scripted_quantized_model.save(\"demo_weight/fbdeit_scripted_quantized.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269\n"
     ]
    }
   ],
   "source": [
    "out = scripted_quantized_model(img)\n",
    "clsidx = torch.argmax(out)\n",
    "print(clsidx.item())\n",
    "# The same output 269 should be printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
    "optimized_scripted_quantized_model = optimize_for_mobile(scripted_quantized_model)\n",
    "optimized_scripted_quantized_model.save(\"demo_weight/fbdeit_optimized_scripted_quantized.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "269\n"
     ]
    }
   ],
   "source": [
    "out = optimized_scripted_quantized_model(img)\n",
    "clsidx = torch.argmax(out)\n",
    "print(clsidx.item())\n",
    "# Again, the same output 269 should be printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_scripted_quantized_model._save_for_lite_interpreter(\"demo_weight/fbdeit_optimized_scripted_quantized_lite.ptl\")\n",
    "ptl = torch.jit.load(\"demo_weight/fbdeit_optimized_scripted_quantized_lite.ptl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:2023-11-19 20:13:03 306593:306593 init.cpp:155] function cbapi->getCuptiStatus() failed with error CUPTI_ERROR_NOT_INITIALIZED (15)\n",
      "WARNING:2023-11-19 20:13:03 306593:306593 init.cpp:156] CUPTI initialization failed - CUDA profiler activities will be missing\n",
      "INFO:2023-11-19 20:13:03 306593:306593 init.cpp:158] If you see CUPTI_ERROR_INSUFFICIENT_PRIVILEGES, refer to https://developer.nvidia.com/nvidia-development-tools-solutions-err-nvgpuctrperm-cupti\n",
      "STAGE:2023-11-19 20:13:03 306593:306593 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2023-11-19 20:13:03 306593:306593 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2023-11-19 20:13:03 306593:306593 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
      "STAGE:2023-11-19 20:13:04 306593:306593 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2023-11-19 20:13:04 306593:306593 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2023-11-19 20:13:04 306593:306593 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
      "STAGE:2023-11-19 20:13:04 306593:306593 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2023-11-19 20:13:04 306593:306593 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2023-11-19 20:13:04 306593:306593 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
      "STAGE:2023-11-19 20:13:04 306593:306593 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2023-11-19 20:13:04 306593:306593 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2023-11-19 20:13:04 306593:306593 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n",
      "STAGE:2023-11-19 20:13:05 306593:306593 ActivityProfilerController.cpp:312] Completed Stage: Warm Up\n",
      "STAGE:2023-11-19 20:13:05 306593:306593 ActivityProfilerController.cpp:318] Completed Stage: Collection\n",
      "STAGE:2023-11-19 20:13:05 306593:306593 ActivityProfilerController.cpp:322] Completed Stage: Post Processing\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original model: 199.71ms\n",
      "scripted model: 91.97ms\n",
      "scripted & quantized model: 100.77ms\n",
      "scripted & quantized & optimized model: 103.52ms\n",
      "lite model: 105.40ms\n"
     ]
    }
   ],
   "source": [
    "with torch.autograd.profiler.profile(use_cuda=False) as prof1:\n",
    "    out = model(img)\n",
    "with torch.autograd.profiler.profile(use_cuda=False) as prof2:\n",
    "    out = scripted_model(img)\n",
    "with torch.autograd.profiler.profile(use_cuda=False) as prof3:\n",
    "    out = scripted_quantized_model(img)\n",
    "with torch.autograd.profiler.profile(use_cuda=False) as prof4:\n",
    "    out = optimized_scripted_quantized_model(img)\n",
    "with torch.autograd.profiler.profile(use_cuda=False) as prof5:\n",
    "    out = ptl(img)\n",
    "\n",
    "print(\"original model: {:.2f}ms\".format(prof1.self_cpu_time_total/1000))\n",
    "print(\"scripted model: {:.2f}ms\".format(prof2.self_cpu_time_total/1000))\n",
    "print(\"scripted & quantized model: {:.2f}ms\".format(prof3.self_cpu_time_total/1000))\n",
    "print(\"scripted & quantized & optimized model: {:.2f}ms\".format(prof4.self_cpu_time_total/1000))\n",
    "print(\"lite model: {:.2f}ms\".format(prof5.self_cpu_time_total/1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Model Inference Time Reduction\n",
      "0                          original model       199.71ms        0%\n",
      "1                          scripted model        91.97ms    53.95%\n",
      "2              scripted & quantized model       100.77ms    49.54%\n",
      "3  scripted & quantized & optimized model       103.52ms    48.17%\n",
      "4                              lite model       105.40ms    47.22%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n        Model                             Inference Time    Reduction\\n0   original model                             1236.69ms           0%\\n1   scripted model                             1226.72ms        0.81%\\n2   scripted & quantized model                  593.19ms       52.03%\\n3   scripted & quantized & optimized model      598.01ms       51.64%\\n4   lite model                                  600.72ms       51.43%\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.DataFrame({'Model': ['original model','scripted model', 'scripted & quantized model', 'scripted & quantized & optimized model', 'lite model']})\n",
    "df = pd.concat([df, pd.DataFrame([\n",
    "    [\"{:.2f}ms\".format(prof1.self_cpu_time_total/1000), \"0%\"],\n",
    "    [\"{:.2f}ms\".format(prof2.self_cpu_time_total/1000),\n",
    "     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof2.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n",
    "    [\"{:.2f}ms\".format(prof3.self_cpu_time_total/1000),\n",
    "     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof3.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n",
    "    [\"{:.2f}ms\".format(prof4.self_cpu_time_total/1000),\n",
    "     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof4.self_cpu_time_total)/prof1.self_cpu_time_total*100)],\n",
    "    [\"{:.2f}ms\".format(prof5.self_cpu_time_total/1000),\n",
    "     \"{:.2f}%\".format((prof1.self_cpu_time_total-prof5.self_cpu_time_total)/prof1.self_cpu_time_total*100)]],\n",
    "    columns=['Inference Time', 'Reduction'])], axis=1)\n",
    "\n",
    "print(df)\n",
    "\n",
    "\"\"\"\n",
    "        Model                             Inference Time    Reduction\n",
    "0   original model                             1236.69ms           0%\n",
    "1   scripted model                             1226.72ms        0.81%\n",
    "2   scripted & quantized model                  593.19ms       52.03%\n",
    "3   scripted & quantized & optimized model      598.01ms       51.64%\n",
    "4   lite model                                  600.72ms       51.43%\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
